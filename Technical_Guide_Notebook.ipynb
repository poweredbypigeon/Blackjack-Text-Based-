{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5lflxy6EhT8V"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poweredbypigeon/Blackjack-Text-Based-/blob/main/Technical_Guide_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Enviornment API"
      ],
      "metadata": {
        "id": "RkfHR0MXShf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Welcome to Environment API**\n",
        "This section explains some of the API for interacting with the game environment. API stands for Application Programming Interface, it details information about the structure of the code so you are aware what information you can obtain from the environment and use within you agent's solution / reward functions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-XAOXXMPTiHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **0.1 Map**\n",
        "\n",
        "The environment map coordinate system is as follows. Some key things to note:\n",
        "\n",
        "1. The center of the map is the origin\n",
        "2. Positive X direction is pointed torwards the right\n",
        "3. Positive Y direction is pointed upwards\n",
        "\n",
        "You can use the image to scale and figure out regions within the game which you can use to define your agent behaviour and/or reward functions!"
      ],
      "metadata": {
        "id": "SAuT7BSOfg7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/lightningminted/UTMIST_AI2_TechnicalGuide/main/env_stage.png\" alt=\"RL Schema\" width=\"900\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "XfmTbB_PrqHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **0.2 Environment and Signals**\n",
        "\n",
        "For accessing variables about the environment (including objects, agents (player, opponent), etc) you can use the following format:"
      ],
      "metadata": {
        "id": "5lflxy6EhT8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agents\n",
        "env.objects[\"player\"]\n",
        "env.objects[\"opponent\"]\n",
        "\n",
        "# Agent Position\n",
        "env.objects[\"player\"].body.position.x   # X position during frame\n",
        "env.objects[\"player\"].body.position.y   # Y position during frame\n",
        "\n",
        "env.objects[\"player\"].body.position.x_change  # Change in x direction position between frames\n",
        "env.objects[\"player\"].body.position.y_change  # Change in y direction position between frames\n",
        "\n",
        "env.objects[\"player\"].body.velocity.x # X velocity of agent\n",
        "env.objects[\"player\"].body.velocity.y # Y velocity of agent\n",
        "\n",
        "# Agent Charachteristics\n",
        "env.objects[\"player\"].DamageTakenTotal      # Integer value of total damage taken\n",
        "env.objects[\"player\"].DamageTakenThisStock  # Integer value of damage taken this stock life\n",
        "env.objects[\"player\"].DamageTakenThisFrame  # Integer value\n",
        "env.objects[\"player\"].WeaponHeldThisFrame   # True or False\n",
        "\n",
        "# Time\n",
        "env.time_elapsed  # Time that has elapsed since start of game\n",
        "env.current_frame # Current frame number\n",
        "\n",
        "# Platforms\n",
        "env.objects['ground']\n",
        "env.objects['platform1']\n",
        "env.objects['platform2']\n",
        "\n",
        "# Signals\n",
        "knockout_signal = Signal()\n",
        "knockout_signal.connect(knockout_reward)\n",
        "knockout_signal.emit(agent=\"player\") # Triggered when an agent is knocked out\n",
        "\n",
        "win_signal = Signal()\n",
        "win_signal.connect(win_reward)\n",
        "win_signal.emit(agent=\"player\") # Triggered when the player wins"
      ],
      "metadata": {
        "id": "wf3VYnJwqOMU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4ebaf6ab-b1fd-4851-a7ae-34e1a9a396dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b22135f1d4f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"player\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"opponent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Agent Position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Reward Function Library"
      ],
      "metadata": {
        "id": "3-7xHeR_nI0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Welcome to the Reward Function Library**\n",
        "\n",
        "This section contains some basic already pre-implemented reward functions for you to use for training your agents! The reward functions that have been implemented correspond to some of the ones in the technical guide. We recommend you use these reward functions as starting points for tweaking and designing your own!\n",
        "\n",
        "As a starting point/hint, weâ€™ve also included some ideas of reward functions that might be good to implement yourself! If you are struggling to implement some of these feel free to ask questions on the discord in the question hub!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lqEUoc88RNDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Existential State/Env Rewards"
      ],
      "metadata": {
        "id": "iybYney2nQCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JEN2lvhFnFG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "69f29c2e-7ba4-445b-f786-bc19328ae8b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Enum' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f8d7a44cb26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRewardMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mASYMMETRIC_OFFENSIVE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSYMMETRIC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mASYMMETRIC_DEFENSIVE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Enum' is not defined"
          ]
        }
      ],
      "source": [
        "class RewardMode(Enum):\n",
        "    ASYMMETRIC_OFFENSIVE = 0\n",
        "    SYMMETRIC = 1\n",
        "    ASYMMETRIC_DEFENSIVE = 2\n",
        "\n",
        "def damage_interaction_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    mode: RewardMode = RewardMode.SYMMETRIC,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on damage interactions between players.\n",
        "\n",
        "    Modes:\n",
        "    - ASYMMETRIC_OFFENSIVE (0): Reward is based only on damage dealt to the opponent\n",
        "    - SYMMETRIC (1): Reward is based on both dealing damage to the opponent and avoiding damage\n",
        "    - ASYMMETRIC_DEFENSIVE (2): Reward is based only on avoiding damage\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        mode (DamageRewardMode): Reward mode, one of DamageRewardMode\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "    # Getting player and opponent from the enviornment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Reward dependent on the mode\n",
        "    damage_taken = player.damage_taken_this_frame\n",
        "    damage_dealt = opponent.damage_taken_this_frame\n",
        "\n",
        "    if mode == RewardMode.ASYMMETRIC_OFFENSIVE:\n",
        "        reward = damage_dealt\n",
        "    elif mode == RewardMode.SYMMETRIC:\n",
        "        reward = damage_dealt - damage_taken\n",
        "    elif mode == RewardMode.ASYMMETRIC_DEFENSIVE:\n",
        "        reward = -damage_taken\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def danger_zone_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    zone_penalty: int = 1,\n",
        "    zone_height: float = 4.2\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Applies a penalty for every time frame player surpases a certain height threshold in the environment.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment.\n",
        "        zone_penalty (int): The penalty applied when the player is in the danger zone.\n",
        "        zone_height (float): The height threshold defining the danger zone.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed penalty as a tensor.\n",
        "    \"\"\"\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    # Apply penalty if the player is in the danger zone\n",
        "    reward = -zone_penalty if player.body.position.y >= zone_height else 0.0\n",
        "\n",
        "    return reward\n"
      ],
      "metadata": {
        "id": "podOwhqGh_Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: that this danger_height being 4.2 corresponds to the following"
      ],
      "metadata": {
        "id": "tsFx657zmJPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/lightningminted/UTMIST_AI2_TechnicalGuide/main/env_stage_dz.png\" alt=\"RL Schema\" width=\"900\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "pDZzjGOPrehu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: This reward function has not been written and is left as an exercise to try and implement\n",
        "#       yourself. Think about the following before implementing:\n",
        "#\n",
        "#       - While having a stock lead is generally good in fighting games,\n",
        "#         how would this reward influence agent behaviour?\n",
        "#       - Is this behaviour even desirable?\n",
        "#       - Is this behaviour more valuable near the beggingin or end of the match,\n",
        "#         and based on that answer how can you change the reward so it considers time?\n",
        "\n",
        "\n",
        "def stock_advantage_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    success_value: float = 0, #TODO\n",
        ") -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the reward given for every time step your agent is edge guarding the opponent.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        success_value (float): Reward value related to having/gaining a weapon (however you define it)\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "    # TODO: Write the function\n",
        "\n",
        "    return reward\n"
      ],
      "metadata": {
        "id": "dprPf1jrY65S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 1.2 Modulo Existential Reward"
      ],
      "metadata": {
        "id": "yTOrlQiboJaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_to_opponent_reward(\n",
        "    env: WarehouseBrawl,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on whether the agent is moving toward the opponent.\n",
        "    The reward is calculated by taking the dot product of the agent's normalized velocity\n",
        "    with the normalized direction vector toward the opponent.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward\n",
        "    \"\"\"\n",
        "    # Getting agent and opponent from the enviornment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Extracting player velocity and position from environment\n",
        "    player_position_dif = np.array([player.body.position.x_change, player.body.position.y_change])\n",
        "\n",
        "    direction_to_opponent = np.array([opponent.body.position.x - player.body.position.x,\n",
        "                                      opponent.body.position.y - player.body.position.y])\n",
        "\n",
        "    # Prevent division by zero or extremely small values\n",
        "    direc_to_opp_norm = np.linalg.norm(direction_to_opponent)\n",
        "    player_pos_dif_norm = np.linalg.norm(player_position_dif)\n",
        "\n",
        "    if direc_to_opp_norm < 1e-6 or player_pos_dif_norm < 1e-6:\n",
        "        return 0.0\n",
        "\n",
        "    # Compute the dot product of the normalized vectors to figure out how much\n",
        "    # current movement (aka velocity) is in alignment with the direction they need to go in\n",
        "    reward = np.dot(player_position_dif / direc_to_opp_norm, direction_to_opponent / direc_to_opp_norm)\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "p-lp6rRhoJL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: This reward function has not been written and is left as an exercise to try and implement\n",
        "#       yourself. Think about the following before implementing:\n",
        "#\n",
        "#       - When does \"edge-guarding\" happen?\n",
        "#       - Where does the oppoent have to be or be moving?\n",
        "#       - Where does your player have to be or be moving?\n",
        "#       - Where does the ledge have to be relative to the agents?\n",
        "\n",
        "def edge_guard_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    success_value: float = 0, #TODO\n",
        "    fail_value: float = 0,    #TODO\n",
        ") -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the reward given for every time step your agent is edge guarding the opponent.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        success_value (float): Reward value for the player hitting first\n",
        "        fail_value (float): Penalty for the opponent hitting first\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "    # TODO: Write the function\n",
        "\n",
        "    return reward\n"
      ],
      "metadata": {
        "id": "tPKhQLZSgz3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Single Event/Sparse Reward"
      ],
      "metadata": {
        "id": "xQJiyQudn_xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Implement an Event/Sparse Reward: Signals**\n",
        "\n",
        "Other types of rewards check every timestep checks if certain conditions are true (a method known as **polling**).\n",
        "\n",
        "Events related to sparse rewards only happen very rarely, so it would be very inefficient to check them every time step. For this reason we introduce something called **signals**.\n",
        "\n",
        "A signal is a way to trigger a response when an event occurs, without needing to constantly check for the event. This is much more efficient because the system only responds when something relevant happens, rather than checking continuously.\n",
        "\n",
        "In this setup, a signal serves as a message or notification that something has occurred in the environment (e.g., a player achieving a knockout).\n",
        "\n",
        "When a signal is emitted, it notifies all functions that have been connected to it (known as **handlers**), and those functions can then take the appropriate action, such as rewarding the agent for the achievement. The reward functions will be handlers in this case."
      ],
      "metadata": {
        "id": "VsDcSbuu2Qr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to implment a signal and a corresponding sparse reward**\n",
        "\n",
        "To make a reward function for sparse rewards you need to write the reward funciton and set up the signal. Below is some code indicating how the signal class is organized - you won't have to modify this asa it is already inside your notebook. What you will have to do is\n",
        "\n",
        "1.   Create an instance of this signal class\n",
        "2.   Write a reward function\n",
        "3.   Connect the reward function to the signal\n",
        "4.   Put the signal emission whenever you want it to activate"
      ],
      "metadata": {
        "id": "nYAsOLoP8chP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Signal class (DO NOT ADD TO NOTEBOOK - already implmented)\n",
        "\n",
        "class Signal:\n",
        "    def __init__(self):\n",
        "        self._handlers = []\n",
        "\n",
        "    def connect(self, handler):\n",
        "        self._handlers.append(handler)\n",
        "\n",
        "    def emit(self, *args, **kwargs):\n",
        "        for handler in self._handlers:\n",
        "            handler(*args, **kwargs)\n",
        "\n",
        "# 1. Create instance of signal (in this case it's knockout)\n",
        "knockout_signal = Signal()\n",
        "\n",
        "# 2. Define the knockout reward (note it is defined in the next cell)\n",
        "\n",
        "# 3. Connect reward functions to signal using .connect\n",
        "knockout_signal.connect(knockout_reward)\n",
        "\n",
        "# 4. Wherever in the code you want the signal to activate, put signal emit,\n",
        "#    including what value you want to pass along to your reward funciton handler\n",
        "\n",
        "knockout_signal.emit(agent=\"player\")    # Signal passing argument \"player\", indicating player was knocked out\n",
        "knockout_signal.emit(agent=\"opponent\")  # Signal passing argument \"opponent\", indicating opponent was knocked out"
      ],
      "metadata": {
        "id": "73aFOJIyyaYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardMode(Enum):\n",
        "    ASYMMETRIC_OPPONENT = 0\n",
        "    SYMMETRIC = 1\n",
        "    ASYMMETRIC_PLAYER = 2\n",
        "\n",
        "\n",
        "def knockout_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    agent: str = \"player\",\n",
        "    mode: RewardMode = RewardMode.SYMMETRIC,\n",
        "    knockout_value_opponent: float = 50.0,\n",
        "    knockout_value_player: float = 50.0,\n",
        "\n",
        "\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on who won the match.\n",
        "\n",
        "    Modes:\n",
        "    - ASYMMETRIC_OPPONENT (0): Reward is based only on the opponent being knocked out\n",
        "    - SYMMETRIC (1): Reward is based on both agents being knocked out\n",
        "    - ASYMMETRIC_PLAYER (2): Reward is based only on your own plauyer being knocked out\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        agent(str): The agent that was knocked out\n",
        "        mode (RewardMode): Reward mode, one of RewardMode\n",
        "        knockout_value_opponent (float): Reward value for knocking out opponent\n",
        "        knockout_value_player (float): Reward penalty for player being knocked out\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "\n",
        "    # Mode logic to compute reward\n",
        "    if mode == RewardMode.ASYMMETRIC_OPPONENT:\n",
        "        if agent == \"opponent\":\n",
        "            reward = knockout_value_opponent # Reward for opponent being knocked out\n",
        "    elif mode == RewardMode.SYMMETRIC:\n",
        "        if agent == \"player\":\n",
        "            reward = -knockout_value_player  # Penalty for player getting knocoked out\n",
        "        elif agent == \"opponent\":\n",
        "            reward = knockout_value_opponent # Reward for opponent being knocked out\n",
        "    elif mode == RewardMode.ASYMMETRIC_PLAYER:\n",
        "        if agent == \"player\":\n",
        "            reward = -knockout_value_player  # Penalty for player getting knocked out\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "vKJGB4bdhbMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def win_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    agent: str = \"player\",\n",
        "    win_value: float = 300.0,\n",
        "    lose_value: float = 200.0,\n",
        ") -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the reward based on knockouts.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        agent(str): The agent that won\n",
        "        win_value (float): Reward value for knocking out opponent\n",
        "        lose_value (float): Reward penalty for player being knocked out\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "\n",
        "    reward = win_value if agent == \"player\" else -lose_value\n",
        "    return reward"
      ],
      "metadata": {
        "id": "EgoOYjZjoDkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: the signal for this reward has not been implemented, this is left as an exercise for\n",
        "#       you to try and implement yourself. Think about when this signal would be activated, and\n",
        "#       how to prevent this signal from being activated again once the first hit has happened\n",
        "\n",
        "def first_hit(\n",
        "    env: WarehouseBrawl,\n",
        "    agent: str = \"player\",\n",
        "    success_value: float = 20.0,\n",
        "    fail_value: float = 10.0,\n",
        ") -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the reward based on who lands the first hit\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        agent (str): The agent that hit first (\"player\" or \"opponent\")\n",
        "        success_value (float): Reward value for the player hitting first\n",
        "        fail_value (float): Penalty for the opponent hitting first\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "\n",
        "    reward = success_value if agent == \"player\" else -fail_value\n",
        "    return reward\n"
      ],
      "metadata": {
        "id": "Bn9Q_h5ONaCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}